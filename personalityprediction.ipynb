{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4ac8f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier, GradientBoostingClassifier\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_importance\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt\n",
    "import pickle as pkl\n",
    "from scipy import sparse\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import re\n",
    "import itertools\n",
    "import string\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Ignore noise warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data_set = pd.read_csv(\"mbti_1.csv\")\n",
    "data_set.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6df919",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcbfeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nRow, nCol = data_set.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda91be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6e656",
   "metadata": {},
   "source": [
    "data_set.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = np.unique(np.array(data_set['type']))\n",
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = data_set.groupby(['type']).count()*50\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1e8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,4))\n",
    "plt.bar(np.array(total.index), height = total['posts'],)\n",
    "plt.xlabel('Personality types', size = 14)\n",
    "plt.ylabel('No. of posts available', size = 14)\n",
    "plt.title('Total posts for each personality type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fcbae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_srs = data_set['type'].value_counts()\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\n",
    "plt.xlabel('Personality types', fontsize=12)\n",
    "plt.ylabel('No. of posts availables', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5331114",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_set.copy()\n",
    "#this function counts the no of words in each post of a user\n",
    "def var_row(row):\n",
    "    l = []\n",
    "    for i in row.split('|||'):\n",
    "        l.append(len(i.split()))\n",
    "    return np.var(l)\n",
    "\n",
    "#this function counts the no of words per post out of the total 50 posts in the whole row\n",
    "df['words_per_comment'] = df['posts'].apply(lambda x: len(x.split())/50)\n",
    "df['variance_of_word_counts'] = df['posts'].apply(lambda x: var_row(x))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.swarmplot(\"type\", \"words_per_comment\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e45fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.jointplot(\"variance_of_word_counts\", \"words_per_comment\", data=df, kind=\"hex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f120dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_jointplot(mbti_type, axs, titles):\n",
    "    df_1 = df[df['type'] == mbti_type]\n",
    "    sns.jointplot(\"variance_of_word_counts\", \"words_per_comment\", data=df_1, kind=\"hex\", ax = axs, title = titles)\n",
    "\n",
    "plt.figure(figsize=(24, 5))    \n",
    "i = df['type'].unique()\n",
    "k = 0\n",
    "\n",
    "for m in range(1,3):\n",
    "  for n in range(1,7):\n",
    "    df_1 = df[df['type'] == i[k]]\n",
    "    sns.jointplot(\"variance_of_word_counts\", \"words_per_comment\", data=df_1, kind=\"hex\" )\n",
    "    plt.title(i[k])\n",
    "    k+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5539380",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"length_posts\"] = df[\"posts\"].apply(len)\n",
    "sns.distplot(df[\"length_posts\"]).set_title(\"Distribution of Lengths of all 50 Posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(df[\"posts\"].apply(lambda x: x.split()))\n",
    "words = [x for y in words for x in y]\n",
    "Counter(words).most_common(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87278248",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = wordcloud.WordCloud(width=1200, height=500, \n",
    "                         collocations=False, background_color=\"white\", \n",
    "                         colormap=\"tab20b\").generate(\" \".join(words))\n",
    "\n",
    "# collocations to False  is set to ensure that the word cloud doesn't appear as if it contains any duplicate words\n",
    "plt.figure(figsize=(25,10))\n",
    "# generate word cloud, interpolation \n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "_ = plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(df['type'].unique()), sharex=True, figsize=(15,len(df['type'].unique())))\n",
    "k = 0\n",
    "for i in df['type'].unique():\n",
    "    df_4 = df[df['type'] == i]\n",
    "    wordcloud = WordCloud(max_words=1628,relative_scaling=1,normalize_plurals=False).generate(df_4['posts'].to_string())\n",
    "    plt.subplot(4,4,k+1)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(i)\n",
    "    ax[k].axis(\"off\")\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca9ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(posts, new_posts):\n",
    "    for post in posts[1].split(\"|||\"):\n",
    "        new_posts.append((posts[0], post))\n",
    "\n",
    "posts = []\n",
    "df.apply(lambda x: extract(x, posts), axis=1)\n",
    "print(\"Number of users\", len(df))\n",
    "print(\"Number of posts\", len(posts))\n",
    "print(\"5 posts from start are:\")\n",
    "posts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "def preprocess_text(df, remove_special=True):\n",
    "    texts = df['posts'].copy()\n",
    "    labels = df['type'].copy()\n",
    "\n",
    "    #Remove links \n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'https?:\\/\\/.*?[\\s+]', '', x.replace(\"|\",\" \") + \" \"))\n",
    "    \n",
    "    #Keep the End Of Sentence characters\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', x + \" \"))\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', x + \" \"))\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', x + \" \"))\n",
    "    \n",
    "    #Strip Punctation\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[\\.+]', \".\",x))\n",
    "\n",
    "    #Remove multiple fullstops\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
    "    \n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','',x))\n",
    "\n",
    "    #Convert posts to lowercase\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: x.lower())\n",
    "\n",
    "    #Remove multiple letter repeating words\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','',x)) \n",
    "\n",
    "    #Remove very short or long words\n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{0,3})?\\b','',x)) \n",
    "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{30,1000})?\\b','',x))\n",
    "    \n",
    "    #Remove MBTI Personality Words - crutial in order to get valid model accuracy estimation for unseen data. \n",
    "    if remove_special:\n",
    "        pers_types = ['INFP' ,'INFJ', 'INTP', 'INTJ', 'ENTP', 'ENFP', 'ISTP' ,'ISFP' ,'ENTJ', 'ISTJ','ENFJ', 'ISFJ' ,'ESTP', 'ESFP' ,'ESFJ' ,'ESTJ']\n",
    "        pers_types = [p.lower() for p in pers_types]\n",
    "        p = re.compile(\"(\" + \"|\".join(pers_types) + \")\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Preprocessing of entered Text\n",
    "new_df = preprocess_text(data_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19accbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_words = 15\n",
    "print(\"Before : Number of posts\", len(new_df)) \n",
    "new_df[\"no. of. words\"] = new_df[\"posts\"].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "new_df = new_df[new_df[\"no. of. words\"] >= min_words]\n",
    "\n",
    "print(\"After : Number of posts\", len(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d7aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1697ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "new_df['type of encoding'] = enc.fit_transform(new_df['type'])\n",
    "\n",
    "target = new_df['type of encoding'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf61e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b631d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english') \n",
    "\n",
    "# Converting posts (or training or X feature) into numerical form by count vectorization\n",
    "train =  vect.fit_transform(new_df[\"posts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0cc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a68521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and evaluating:60-40\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.4, stratify=target, random_state=42)\n",
    "print ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "\n",
    "#Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state = 1)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracies['Random Forest'] = accuracy* 100.0 \n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4ae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "\n",
    "Y_pred = xgb.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracies['XG Boost'] = accuracy* 100.0\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e5c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(max_iter=5, tol=None)\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "Y_pred = sgd.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracies['Gradient Descent'] = accuracy* 100.0\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a05cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "Y_pred = logreg.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracies['Logistic Regression'] = accuracy* 100.0\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02440811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "Y_pred = knn.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracies['KNN'] = accuracy* 100.0\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "#try to find best k value\n",
    "scoreList = []\n",
    "for i in range(1,20):\n",
    "    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n",
    "    knn2.fit(X_train, y_train)\n",
    "    scoreList.append(knn2.score(X_test, y_test))\n",
    "\n",
    "plt.plot(range(1,20), scoreList)\n",
    "plt.xticks(np.arange(1,20,1))\n",
    "plt.xlabel(\"K value\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n",
    "\n",
    "acc = max(scoreList)*100\n",
    "\n",
    "print(\"Maximum KNN Score is {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(random_state = 1)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "Y_pred = svm.predict(X_test)\n",
    "\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracies['SVM'] = accuracy* 100.0\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15234c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(accuracies, orient='index', columns=['Accuracies(%)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and evaluating 70-30 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.33, stratify=target, random_state=42)\n",
    "print ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472a854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "\n",
    "#Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state = 1)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracies['Random Forest'] = accuracy* 100.0 \n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a8e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "\n",
    "Y_pred = xgb.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracies['XG Boost'] = accuracy* 100.0\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17744d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(max_iter=5, tol=None)\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "Y_pred = sgd.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracies['Gradient Descent'] = accuracy* 100.0\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "Y_pred = logreg.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracies['Logistic Regression'] = accuracy* 100.0\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cb979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "Y_pred = knn.predict(X_test)\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracies['KNN'] = accuracy* 100.0\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "scoreList = []\n",
    "for i in range(1,20):\n",
    "    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n",
    "    knn2.fit(X_train, y_train)\n",
    "    scoreList.append(knn2.score(X_test, y_test))\n",
    "    \n",
    "plt.plot(range(1,20), scoreList)\n",
    "plt.xticks(np.arange(1,20,1))\n",
    "plt.xlabel(\"K value\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n",
    "\n",
    "acc = max(scoreList)*100\n",
    "\n",
    "print(\"Maximum KNN Score is {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79339244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(random_state = 1)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "Y_pred = svm.predict(X_test)\n",
    "\n",
    "predictions = [round(value) for value in Y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracies['SVM'] = accuracy* 100.0\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4914bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing algorithms \n",
    "pd.DataFrame.from_dict(accuracies, orient='index', columns=['Accuracies(%)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#four classifiers across MBTI axis\n",
    "data = pd.read_csv(\"mbti_1.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed9230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_types(row):\n",
    "    t=row['type']\n",
    "\n",
    "    I = 0; N = 0\n",
    "    T = 0; J = 0\n",
    "    \n",
    "    if t[0] == 'I': I = 1\n",
    "    elif t[0] == 'E': I = 0\n",
    "    else: print('I-E not found') \n",
    "        \n",
    "    if t[1] == 'N': N = 1\n",
    "    elif t[1] == 'S': N = 0\n",
    "    else: print('N-S not found')\n",
    "        \n",
    "    if t[2] == 'T': T = 1\n",
    "    elif t[2] == 'F': T = 0\n",
    "    else: print('T-F not found')\n",
    "    \n",
    "    if t[3] == 'J': J = 1\n",
    "    elif t[3] == 'P': J = 0\n",
    "    else: print('J-P not found')\n",
    "    return pd.Series( {'IE':I, 'NS':N , 'TF': T, 'JP': J }) \n",
    "\n",
    "data = data.join(data.apply (lambda row: get_types (row),axis=1))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dcaa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Introversion (I) /  Extroversion (E):\\t\", data['IE'].value_counts()[0], \" / \", data['IE'].value_counts()[1])\n",
    "print (\"Intuition (N) / Sensing (S):\\t\\t\", data['NS'].value_counts()[0], \" / \", data['NS'].value_counts()[1])\n",
    "print (\"Thinking (T) / Feeling (F):\\t\\t\", data['TF'].value_counts()[0], \" / \", data['TF'].value_counts()[1])\n",
    "print (\"Judging (J) / Perceiving (P):\\t\\t\", data['JP'].value_counts()[0], \" / \", data['JP'].value_counts()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba678648",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "bottom = (data['IE'].value_counts()[0], data['NS'].value_counts()[0], data['TF'].value_counts()[0], data['JP'].value_counts()[0])\n",
    "top = (data['IE'].value_counts()[1], data['NS'].value_counts()[1], data['TF'].value_counts()[1], data['JP'].value_counts()[1])\n",
    "\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "# the width of the bars\n",
    "width = 0.7           # or len(x) can also be used here\n",
    "\n",
    "p1 = plt.bar(ind, bottom, width, label=\"I, N, T, F\")\n",
    "p2 = plt.bar(ind, top, width, bottom=bottom, label=\"E, S, F, P\") \n",
    "\n",
    "plt.title('Distribution accoss types indicators')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(ind, ('I / E',  'N / S', 'T / F', 'J / P',))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e9733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['IE','NS','TF','JP']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.RdBu\n",
    "corr = data[['IE','NS','TF','JP']].corr()\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.title('Features Correlation Heatmap', size=15)\n",
    "sns.heatmap(corr, cmap=cmap,  annot=True, linewidths=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f0ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing stage\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "# Remove the stop words for speed \n",
    "useless_words = stopwords.words(\"english\")\n",
    "\n",
    "# Remove these from the posts\n",
    "unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "unique_type_list = [x.lower() for x in unique_type_list]\n",
    "\n",
    "# Or we can use Label Encoding (as above) of this unique personality type indicator list\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "#        'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "# lab_encoder = LabelEncoder().fit(unique_type_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the MBTI personality into 4 letters and binarizing it\n",
    "\n",
    "b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "b_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n",
    "\n",
    "def translate_personality(personality):\n",
    "    # transform mbti to binary vector\n",
    "    return [b_Pers[l] for l in personality]\n",
    "\n",
    "#To show result output for personality prediction\n",
    "def translate_back(personality):\n",
    "    # transform binary vector to mbti personality\n",
    "    s = \"\"\n",
    "    for i, l in enumerate(personality):\n",
    "        s += b_Pers_list[i][l]\n",
    "    return s\n",
    "\n",
    "list_personality_bin = np.array([translate_personality(p) for p in data.type])\n",
    "print(\"Binarize MBTI list: \\n%s\" % list_personality_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('omw-1.4')\n",
    "def pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True):\n",
    "  list_personality = []\n",
    "  list_posts = []\n",
    "  len_data = len(data)\n",
    "  i=0\n",
    "  \n",
    "  for row in data.iterrows():\n",
    "      # check code working \n",
    "      # i+=1\n",
    "      # if (i % 500 == 0 or i == 1 or i == len_data):\n",
    "      #     print(\"%s of %s rows\" % (i, len_data))\n",
    "\n",
    "      #Remove and clean comments\n",
    "      posts = row[1].posts\n",
    "\n",
    "      #Remove url links \n",
    "      temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n",
    "\n",
    "      #Remove Non-words - keep only words\n",
    "      temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "        # Remove spaces > 1\n",
    "      temp = re.sub(' +', ' ', temp).lower()\n",
    "\n",
    "      #Remove multiple letter repeating words\n",
    "      temp = re.sub(r'([a-z])\\1{2,}[\\s|\\w]*', '', temp)\n",
    "\n",
    "      #Remove stop words\n",
    "      if remove_stop_words:\n",
    "          temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in useless_words])\n",
    "      else:\n",
    "          temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n",
    "          \n",
    "      #Remove MBTI personality words from posts\n",
    "      if remove_mbti_profiles:\n",
    "          for t in unique_type_list:\n",
    "              temp = temp.replace(t,\"\")\n",
    "                # transform mbti to binary vector\n",
    "      type_labelized = translate_personality(row[1].type) #or use lab_encoder.transform([row[1].type])[0]\n",
    "      list_personality.append(type_labelized)\n",
    "      # the cleaned data temp is passed here\n",
    "      list_posts.append(temp)\n",
    "\n",
    "  # returns the result\n",
    "  list_posts = np.array(list_posts)\n",
    "  list_personality = np.array(list_personality)\n",
    "  return list_posts, list_personality\n",
    "\n",
    "list_posts, list_personality  = pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True)\n",
    "print(\"Example :\")\n",
    "print(\"\\nPost before preprocessing:\\n\\n\", data.posts[0])\n",
    "print(\"\\nPost after preprocessing:\\n\\n\", list_posts[0])\n",
    "print(\"\\nMBTI before preprocessing:\\n\\n\", data.type[0])\n",
    "print(\"\\nMBTI after preprocessing:\\n\\n\", list_personality[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa2621",
   "metadata": {},
   "outputs": [],
   "source": [
    "nRow, nCol = list_personality.shape\n",
    "print(f'No. of posts = {nRow}  and No. of Personalities = {nCol} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering\n",
    "# Vectorizing the database posts to a matrix of token counts for the model\n",
    "cntizer = CountVectorizer(analyzer=\"word\", \n",
    "                             max_features=1000,  \n",
    "                             max_df=0.7,\n",
    "                             min_df=0.1) \n",
    "# the feature should be made of word n-gram \n",
    "# Learn the vocabulary dictionary and return term-document matrix\n",
    "print(\"Using CountVectorizer :\")\n",
    "X_cnt = cntizer.fit_transform(list_posts)\n",
    "\n",
    "#The enumerate object yields pairs containing a count and a value (useful for obtaining an indexed list)\n",
    "feature_names = list(enumerate(cntizer.get_feature_names()))\n",
    "print(\"10 feature names can be seen below\")\n",
    "print(feature_names[0:10])\n",
    "tfizer = TfidfTransformer()\n",
    "# Learn the idf vector (fit) and transform a count matrix to a tf-idf representation\n",
    "print(\"\\nUsing Tf-idf :\")\n",
    "\n",
    "print(\"Now the dataset size is as below\")\n",
    "X_tfidf =  tfizer.fit_transform(X_cnt).toarray()\n",
    "print(X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_dic = {}\n",
    "for key in cntizer.vocabulary_:\n",
    "    reverse_dic[cntizer.vocabulary_[key]] = key\n",
    "top_10 = np.asarray(np.argsort(np.sum(X_cnt, axis=0))[0,-10:][0, ::-1]).flatten()\n",
    "top_50 = np.asarray(np.argsort(np.sum(X_cnt, axis=0))[0,-50:][0, ::-1]).flatten()\n",
    "[reverse_dic[v] for v in top_50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810aa34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting x and y variable\n",
    "personality_type = [ \"IE: Introversion (I) / Extroversion (E)\", \"NS: Intuition (N) / Sensing (S)\", \n",
    "                   \"FT: Feeling (F) / Thinking (T)\", \"JP: Judging (J) / Perceiving (P)\"  ]\n",
    "\n",
    "for l in range(len(personality_type)):\n",
    "    print(personality_type[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0112d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X: 1st posts in tf-idf representation\\n%s\" % X_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c5c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For MBTI personality type : %s\" % translate_back(list_personality[0,:]))\n",
    "print(\"Y : Binarized MBTI 1st row: %s\" % list_personality[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a4de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and evaluating data\n",
    "# Posts in tf-idf representation\n",
    "X = X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55526c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest model for MBTI dataset\n",
    "# Individually training each mbti personlity type\n",
    "for l in range(len(personality_type)):\n",
    "    \n",
    "    Y = list_personality[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "    # fit model on training data\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc7b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost model for MBTI dataset \n",
    "# Individually training each mbti personlity type\n",
    "for l in range(len(personality_type)):\n",
    "    \n",
    "    Y = list_personality[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "    # fit model on training data\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocastic Gradient Descent for MBTI dataset\n",
    "# Individually training each mbti personlity type\n",
    "for l in range(len(personality_type)):\n",
    "\n",
    "    Y = list_personality[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "    # fit model on training data\n",
    "    model = SGDClassifier() \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5996b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression for MBTI dataset\n",
    "# Individually training each mbti personlity type\n",
    "for l in range(len(personality_type)):\n",
    "\n",
    "    Y = list_personality[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "    # fit model on training data\n",
    "    model = LogisticRegression() \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879cc9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 KNN model for MBTI dataset\n",
    "# Individually training each mbti personlity type\n",
    "for l in range(len(personality_type)):\n",
    "\n",
    "    Y = list_personality[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "    # fit model on training data\n",
    "    model = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "   \n",
    "    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM model for MBTI dataset\n",
    "# Individually training each mbti personlity type\n",
    "for l in range(len(personality_type)):\n",
    "    \n",
    "    Y = list_personality[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "    # fit model on training data\n",
    "    model = SVC(random_state = 1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc58bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "\n",
    "param['n_estimators'] = 200 #100\n",
    "param['max_depth'] = 2 #3\n",
    "param['nthread'] = 8 #1\n",
    "param['learning_rate'] = 0.2 #0.1\n",
    "\n",
    "# Individually training each mbti personlity type\n",
    "for l in range(len(personality_type)):\n",
    "    Y = list_personality[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    seed = 7\n",
    "    test_size = 0.33\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    # fit model on training data\n",
    "    model = XGBClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    # make predictions for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7bdac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_posts  = \"\"\" Hi I am 21 years, currently, I am pursuing my graduate degree in computer science and management (Mba Tech CS ), It is a 5-year dual degree.... My CGPA to date is 3.8/4.0 . I have a passion for teaching since childhood. Math has always been the subject of my interest in school. Also, my mother has been one of my biggest inspirations for me. She started her career as a teacher and now has her own education trust with preschools schools in Rural and Urban areas. During the period of lockdown, I dwelled in the field of blogging and content creation on Instagram.  to spread love positivity kindness . I hope I am able deliver my best to the platform and my optimistic attitude helps in the growth that is expected. Thank you for the opportunity. \"\"\"\n",
    "\n",
    "# The type is just a dummy so that the data prep function can be reused\n",
    "mydata = pd.DataFrame(data={'type': ['INFJ'], 'posts': [my_posts]})\n",
    "\n",
    "my_posts, dummy  = pre_process_text(mydata, remove_stop_words=True, remove_mbti_profiles=True)\n",
    "\n",
    "my_X_cnt = cntizer.transform(my_posts)\n",
    "my_X_tfidf =  tfizer.transform(my_X_cnt).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4517f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "param['n_estimators'] = 200\n",
    "param['max_depth'] = 2\n",
    "param['nthread'] = 8\n",
    "param['learning_rate'] = 0.2\n",
    "\n",
    "#XGBoost model for MBTI dataset\n",
    "result = []\n",
    "# Individually training each mbti personlity type\n",
    "for l in range(len(personality_type)):\n",
    "    print(\"%s classifier trained\" % (personality_type[l]))\n",
    "    \n",
    "    Y = list_personality[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "    # fit model on training data\n",
    "    model = XGBClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions for my  data\n",
    "    y_pred = model.predict(my_X_tfidf)\n",
    "    result.append(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The result is: \", translate_back(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4685c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_posts = \"\"\" They act like they care They tell me to share But when I carve the stories on my arm The doctor just calls it self harm I’m not asking for attention There’s a reason I have apprehensions I just need you to see What has become of me||| I know I’m going crazy But they think my thoughts are just hazy When in that chaos, in that confusion I’m crying out for help, to escape my delusions||| Mental health is a state of mind How does one keep that up when assistance is denied All my failed attempts to fight the blaze You treat it like its a passing phase||| Well stop, its not, because mental illness is real Understand that we’re all not made of steel Because when you brush these issues under the carpet You make it seem like its our mistake we’re not guarded||| Don’t you realise that its a problem that needs to be addressed Starting at home, in our nest Why do you keep your mouths shut about such things Instead of caring for those with broken wings||| What use is this social stigma When mental illness is not even such an enigma Look around and you’ll see the numbers of the affected hiding under the covers ||| This is an issue that needs to be discussed Not looked down upon with disgust Mental illness needs to be accepted So that people can be protected ||| Let me give you some direction People need affection The darkness must be escaped Only then the lost can be saved||| Bring in a change Something not very strange The new year is here Its time to eradicate fear||| Recognise the wrists under the knives To stop mental illness from taking more lives Let’s break the convention Start ‘suicide prevention’.||| Hoping the festival of lights drives the darkness of mental illness away\"\"\"\n",
    "mydata = pd.DataFrame(data={'type': ['INFP'], 'posts': [my_posts]})\n",
    "my_posts, dummy  = pre_process_text(mydata, remove_stop_words=True, remove_mbti_profiles=True)\n",
    "my_X_cnt = cntizer.transform(my_posts)\n",
    "my_X_tfidf =  tfizer.transform(my_X_cnt).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "param['n_estimators'] = 200\n",
    "param['max_depth'] = 2\n",
    "param['nthread'] = 8\n",
    "param['learning_rate'] = 0.2\n",
    "\n",
    "#XGBoost model for MBTI dataset\n",
    "result = []\n",
    "# Individually training each mbti personlity type\n",
    "for l in range(len(personality_type)):\n",
    "    print(\"%s classifier trained\" % (personality_type[l]))\n",
    "    \n",
    "    Y = list_personality[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "    # fit model on training data\n",
    "    model = XGBClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions for my  data\n",
    "    y_pred = model.predict(my_X_tfidf)\n",
    "    result.append(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_posts  = \"\"\" I dont think anyone would be able to live 300 years i am not talking about the physical ability to do so but the mental fortitude unless you decide to live away from civilization it simply is not possible.|||Believe me you would not want to live for that long alone , unless there are others who can live for 300 years as well.|||You cannot enjoy something if you say something to yourself like ‘I wanna enjoy this , i think this thing is gonna be fun’ believe me it doesn’t work.|||I think this problem might be face by a lot of people.|||Firstly you should only study stuff that interests you . (obvious)|||Now there are subjects that you school forces you to take and you have no option but to some how score in those subjects. (What i used to think is subjects like history , geography and most of all Hindi are utterly useless , i wanna be a programmer why do i study these)|||But because i had no choice i thought lets give these a try. I started questioning things and when i dug deep into the events of history and and why each event took place or how it was of benefit to the benefactor. This questioning and finding out the reasons made me like history.|||My point is unless ya’ll start questioning and researching further than whats necessary for exams you wont like that subject. All the subjects are beautiful , its what you choose to see. Basically give everything a real shot in life , everything works out. (my advice seems ironical as if you dont like the subject and i am telling you to research further but try it once )|||And also never study because there is exam or because you have to do an assignment or because someone is told you to or because ….|||But please ONLY STUDY BECAUSE YOU FEEL LIKE AND BECAUSE YOU WANT TO. Until you develop this sense of want to study it will be hard for you to like it. You must like it so much , that you know when people say after studying maths for 1 hour they took a break of 20 mins watching their favorite tv show(lets say friends)|||For you it should be like after 1 hour of x(that you dont like but HAVE to do) activity you take a break of 20 mins and you study , (like i like reading article on ai so i do that) you might like bio you will do that what i want to say is that is what it means to like something and only then you can truly enjoy it.|||If only something known as luck existed. (no offence to the readers or person who asked the question)|||Luck is a really interesting term , a really complex illusion. What i am saying is there is nothing known as luck that exists. Something simply doesnt just happen. It happens for a reason and with a reason.|||Some over here might claim that if it is not luck then what is it that cause (cause a child to be born in a rich family or a person to be saved by weirdest phenomenon and escape death.)|||What i want ya’ll to know is firstly that being born rich cannot be called as “lucky” like we cannot say to be born in a rich family is particularly a good thing there are many reasons to this (some people like to work their way up , Some want to experience the life troubles, well whatever the reasons might be) So firstly when we say something to be lucky we just CONSIDER that what happened was good. Same goes with the case of being saved from and awful accident. We still dont know the purpose of life or our existence and hence we dont know if living is a boon. this might be harsh for some but Reality is Harsh.|||What want ya’ll to know is never feel bad if something good(in your perspective) happens to someone as It might as well not turn out to be good if you see the BIG picture.|||Besides its also a good thing to think this way as its boosts up your hopes , like you might consider that everything that has happened to you has made you what you are and even if you don’t appreciate your conditions there is someone somewhere who would want to be in that.|||I think the all of us are 100% selfish. (no offence)|||The thing is even if we say we care about someone and then we help that person in reality we are just making ourselves happy by helping that someone.|||What i mean to say is even when people talk about sacrifices for others the reality is that sacrifice made that person or those persons happy which you cared for and thus those people being happy makes you happy.|||Everything comes down to you. You can try and deny it but you all know it.|||Now about those people who sacrifice their lives for others that is a peculiar case , and here too (this might be hard for some to believe) but they sacrifice life for someone they loved (they thought they loved) but the truth is in a situation where a person sacrifices his life for another the truth is that if he hadnt dont that he couldn’t have survived without that person anyways and then there are always some who seek glory.|||I hope you get the point. Even when you say that people spend 30s and 40s the truth is making their kids life perfect gives them happiness. There are people without kids too cause for these making their kids life perfect doesnt give them as much happiness as focusing on their own goals might.|||Now i believe there might be many who thought that making kids life perfect might give them happiness but it turns out to be false and then they are stuck there fulfilling moral obligations. It all comes down to your resolves and how firm you are in you decisions.|||Isn’t it fun to watch our disciples fight among themselves to prove that only one of us exists!|||I tend to believe that everything in this universe HAS TO HAVE A PURPOSE. Rather than thinking that the universe is a useless place and we have no purpose i would rather think we are just too stupid and dont know or cannot find the purpose. I have always wondered that what would be our reason to exist , once i thought of us (humans on earth) a crop created by aliens that takes this long to grow (i mean may be it would be not possible to create humans by a process other than evolution) so the aliens started the life on earth and are now just waiting for us to evolve , and so our purpose according to that is nothing but to serve as food or may be what ever they want , now then the question arises is what would their purpose be , And all such hypothetical situation lead to to scenario to go into infinity.|||After a lot of such crazy thought i came to 2 conclusions , 1 is pretty simple our purpose of life is find a purpose for our lives , and the weird thing is unlike other things once you find what your were looking for , the process ends there. The paradox is after you find your purpose of life did your succeed in finishing your purpose of life or did you just begun ?|||The 2nd one is what most people should agree is happiness. Now I think that this happiness should be confined to YOUR’s and ONLY your Happiness. Now the thing is some people gain happiness by giving people happiness.|||Let happiness be a quantifiable entity. We shall say that we start 0 oh hp . |||The zone where we dont feel happy or sad is 0 . Anything above 0 means you are happy and less than 0 is you are sad .|||Now one might think that if we suppose attain 100 hp , and do nothing after that we shall remain happy , the problem is that after a while our bar above which we remain moves up (simple adaptation) so now anything below 100hp is sad . This is the main problem with happiness , and So we need to keep doing stuff and increasing our hp . So maybe this counts as a purpose of life.|||Like when we are told about those saints and stuff who abandon society and live their life alone in discrete places where no one can disturb them. I believe the simple reason for this they have found happiness in doing nothing . Like someone finds happiness in making more money (there is a possibility that you might not make and hence be sad ) So these saint type people do something that has no opposite , like i know when they do nothing (i know its opposite is everything) and thats the very reason they go away from the civilization where they literally will have to do nothing . I also tend to think of these people as cowards who fear that they might loose at point.|||But the very point arises is what did you gain by gaining happiness. So again there is no end so Keep chasing the infinity its pointless but Keeps you busy(till the point you are alive) then after you are dead i guess nothing matters.|||But anyways thats the most easy thing to obtain happiness without sadness.|||But the very point arises is what did you gain by gaining happiness. So again there is no end so Keep chasing the infinity its pointless but Keeps you busy(till the point you are alive) then after you are dead i guess nothing matters.|||But if the life after death theory is true , and if by chance are memories are also transferred every time we are reborn. We all are fucked We are for infinity stuck chasing the infinity . (I wonder what happens when infinity chases infinity It will be fun to watch !) |||I think rather than worrying about these things we should just enjoy life . Because there will always something that we dont know that we dont know and thus we shall never know that. this is just 2 loops of not know , you can reach it , but Do it infinitely . Infinity is a bitch.|||Well To all beings good luck finding a purpose and to all those who know that it doesnt exist ya’ll are fucked.\"\"\"\n",
    "mydata = pd.DataFrame(data={'type': ['ENTP'], 'posts': [my_posts]})\n",
    "my_posts, dummy  = pre_process_text(mydata, remove_stop_words=True, remove_mbti_profiles=True)\n",
    "my_X_cnt = cntizer.transform(my_posts)\n",
    "my_X_tfidf =  tfizer.transform(my_X_cnt).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d28eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "param['n_estimators'] = 200\n",
    "param['max_depth'] = 2\n",
    "param['nthread'] = 8\n",
    "param['learning_rate'] = 0.2\n",
    "\n",
    "#XGBoost model for MBTI dataset\n",
    "result = []\n",
    "# Individually training each mbti personlity type\n",
    "for l in range(len(personality_type)):\n",
    "    print(\"%s classifier trained\" % (personality_type[l]))\n",
    "    \n",
    "    Y = list_personality[:,l]\n",
    "\n",
    "    # split data into train and test sets\n",
    "    seed = 7\n",
    "    test_size = 0.33\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "    # fit model on training data\n",
    "    model = XGBClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions for my  data\n",
    "    y_pred = model.predict(my_X_tfidf)\n",
    "    result.append(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65169e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The result is: \", translate_back(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb5cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
